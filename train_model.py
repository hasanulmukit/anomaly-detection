{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# train_model.py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nimport pickle\n\n# For evaluation metrics:\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n\n# -----------------------------\n# 1. Load and Preprocess the Data\n# -----------------------------\n# Replace 'blockchain_data.csv' with your actual CSV file name.\ndf = pd.read_csv(\"blockchain_data.csv\")\n\n# Convert timestamp column to datetime (assuming timestamp is in milliseconds)\ndf['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n\n# -----------------------------\n# 2. Feature Engineering\n# -----------------------------\n# We will use these columns as features:\n#   size, tx_count, difficulty, median_fee_rate, avg_fee_rate, total_fees,\n#   fee_range_min, fee_range_max, input_count, output_count, output_amount\n# Additionally, we compute a new feature: fee_spread = fee_range_max - fee_range_min.\nfeatures = ['size', 'tx_count', 'difficulty', 'median_fee_rate', 'avg_fee_rate', \n            'total_fees', 'fee_range_min', 'fee_range_max', 'input_count', 'output_count', 'output_amount']\n\ndf['fee_spread'] = df['fee_range_max'] - df['fee_range_min']\nfeatures.append('fee_spread')\n\n# -----------------------------\n# 3. Feature Scaling\n# -----------------------------\nscaler = MinMaxScaler()\nX = scaler.fit_transform(df[features])\n\n# Save the scaler to use later in the dashboard.\nwith open('scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# -----------------------------\n# 4. Train/Test Split\n# -----------------------------\n# For an unsupervised autoencoder, we assume the dataset is mostly normal.\nX_train, X_val = train_test_split(X, test_size=0.2, random_state=42)\n\n# -----------------------------\n# 5. Build the Autoencoder Model\n# -----------------------------\ninput_dim = X_train.shape[1]\nencoding_dim = 8  # This is a hyperparameter; feel free to experiment.\n\n# Encoder\ninput_layer = Input(shape=(input_dim,))\nencoder = Dense(16, activation=\"relu\")(input_layer)\nencoder = Dense(encoding_dim, activation=\"relu\")(encoder)\n\n# Decoder\ndecoder = Dense(16, activation=\"relu\")(encoder)\ndecoder = Dense(input_dim, activation=\"sigmoid\")(decoder)\n\nautoencoder = Model(inputs=input_layer, outputs=decoder)\nautoencoder.compile(optimizer='adam', loss='mse')\nautoencoder.summary()\n\n# -----------------------------\n# 6. Train the Autoencoder\n# -----------------------------\nhistory = autoencoder.fit(X_train, X_train,\n                           epochs=50,\n                           batch_size=32,\n                           validation_data=(X_val, X_val),\n                           shuffle=True)\n\n# -----------------------------\n# 7. Determine the Anomaly Threshold\n# -----------------------------\n# Compute the reconstruction error on the training set.\nreconstructions = autoencoder.predict(X_train)\nmse_train = np.mean(np.power(X_train - reconstructions, 2), axis=1)\nthreshold = np.mean(mse_train) + 3 * np.std(mse_train)\nprint(\"Reconstruction error threshold:\", threshold)\n\n# Save the trained autoencoder model.\nautoencoder.save('autoencoder_model.h5')\n\n# Save the threshold for later use.\nwith open('threshold.pkl', 'wb') as f:\n    pickle.dump(threshold, f)\n\n# -----------------------------\n# 8. Evaluate the Model on Synthetic Anomalies in the Validation Set\n# -----------------------------\n# For evaluation purposes, we inject synthetic anomalies into a copy of the validation set.\n# This provides us with ground truth labels to compare performance.\n\n# Create a copy of X_val for testing and initialize ground truth labels (0 = normal)\nX_val_test = X_val.copy()\ny_true = np.zeros(X_val.shape[0], dtype=int)\n\n# Inject synthetic anomalies into 10% of the validation set\nn_val = X_val.shape[0]\nn_anomalies = int(0.1 * n_val)  # 10% anomalies\nanomaly_indices = np.random.choice(n_val, size=n_anomalies, replace=False)\n\n# For each selected anomaly index, multiply all features by a factor (e.g., 5)\nfor idx in anomaly_indices:\n    X_val_test[idx] = X_val_test[idx] * 5\n    y_true[idx] = 1\n\n# Get reconstructions and compute the reconstruction error on the test validation set\nreconstructions_val = autoencoder.predict(X_val_test)\nmse_val = np.mean(np.power(X_val_test - reconstructions_val, 2), axis=1)\n\n# Generate predictions: mark as anomaly if reconstruction error exceeds the threshold\ny_pred = (mse_val > threshold).astype(int)\n\n# Compute evaluation metrics\nprecision = precision_score(y_true, y_pred, zero_division=0)\nrecall = recall_score(y_true, y_pred, zero_division=0)\nf1 = f1_score(y_true, y_pred, zero_division=0)\ncm = confusion_matrix(y_true, y_pred)\ntry:\n    roc_auc = roc_auc_score(y_true, mse_val)\nexcept Exception as e:\n    roc_auc = None\n\nprint(\"\\nEvaluation Metrics on Synthetic Validation Set:\")\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\nprint(\"Confusion Matrix:\\n\", cm)\nprint(\"ROC AUC Score:\", roc_auc)\n","metadata":{"_uuid":"9a8165f1-a77f-4a06-baf2-a88641c58a77","_cell_guid":"d027e76d-76ed-4523-95b4-1846cd0504d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}